{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MySoftwares\\Anaconda\\envs\\data\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from EduNLP.Pretrain import DisenQTokenizer, train_disenqnet\n",
    "from EduNLP.Vector import DisenQModel, T2V\n",
    "from EduNLP.I2V import DisenQ, get_pretrained_i2v\n",
    "from EduNLP.ModelZoo import load_items\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\"\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练自己的disenQNet模型\n",
    "## 1. 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../..\"\n",
    "\n",
    "data_dir = f\"{BASE_DIR}/static/test_data\"\n",
    "output_dir = f\"{BASE_DIR}/examples/test_model/data/disenq\"\n",
    "\n",
    "disen_data_train = load_items(f\"{data_dir}/disenq_train.json\")\n",
    "disen_data_test = load_items(f\"{data_dir}/disenq_test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 训练和评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load vocab from ../../examples/test_model/data/disenq\\vocab.list\n",
      "load concept from ../../examples/test_model/data/disenq\\concept.list\n",
      "load word2vec from ../../examples/test_model/data/disenq\\wv.th\n",
      "processing raw data for QuestionDataset...\n",
      "vocab size: 6827\n",
      "concept size: 5\n",
      "load vocab from ../../examples/test_model/data/disenq\\vocab.list\n",
      "load concept from ../../examples/test_model/data/disenq\\concept.list\n",
      "load word2vec from ../../examples/test_model/data/disenq\\wv.th\n",
      "processing raw data for QuestionDataset...\n",
      "Start training the disenQNet...\n",
      "[Epoch  1] train loss: 1.5524\n",
      "[Epoch  2] train loss: 1.5753, eval loss: 1.5866\n",
      "[Epoch  3] train loss: 1.4864, eval loss: 1.4997\n",
      "[Epoch  4] train loss: 1.4471, eval loss: 1.4667\n",
      "[Epoch  5] train loss: 1.3798, eval loss: 1.4123\n",
      "[Epoch  6] train loss: 1.3078, eval loss: 1.3586\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DisenQTokenizer(max_length=250, tokenize_method=\"space\")\n",
    "\n",
    "train_params = {\n",
    "    # data params\n",
    "    \"trim_min\": 2,\n",
    "    \"w2v_workers\": 1,\n",
    "    # model params\n",
    "    \"hidden\": 128,\n",
    "    \"dropout\": 0.2,\n",
    "    \"pos_weight\": 1,\n",
    "    \"cp\": 1.5,\n",
    "    \"mi\": 1.0,\n",
    "    \"dis\": 2.0,\n",
    "    # training params\n",
    "    \"epoch\": 5,\n",
    "    \"batch\": 64,\n",
    "    \"lr\": 1e-3,\n",
    "    \"step\": 20,\n",
    "    \"gamma\": 0.5,\n",
    "    \"warm_up\": 1,\n",
    "    \"adv\": 10,\n",
    "    \"device\": \"cuda\"\n",
    "}\n",
    "data_formation = {\n",
    "    \"content\": \"content\",\n",
    "    \"knowledge\": \"knowledge\"\n",
    "}\n",
    "train_disenqnet(\n",
    "    disen_data_train,\n",
    "    tokenizer,\n",
    "    output_dir,\n",
    "    output_dir,\n",
    "    train_params=train_params,\n",
    "    test_items=disen_data_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.使用模型\n",
    "\n",
    "### 3.1 使用I2V将题目转为向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 23, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([1, 11, 128])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"tokenizer_config_dir\": output_dir,\n",
    "}\n",
    "i2v = DisenQ('disenq', 'disenq', output_dir, tokenizer_kwargs=tokenizer_kwargs, device=\"cuda\")\n",
    "\n",
    "test_items = [\n",
    "    {\"content\": \"10 米 的 (2/5) = 多少 米 的 (1/2),有 公 式\"},\n",
    "    {\"content\": \"10 米 的 (2/5) = 多少 米 的 (1/2),有 公 式 , 如 图 , 若 $x,y$ 满 足 约 束 条 件 公 式\"},\n",
    "]\n",
    "\n",
    "t_vec = i2v.infer_token_vector(test_items, key=lambda x: x[\"content\"])\n",
    "i_vec_k = i2v.infer_item_vector(test_items, key=lambda x: x[\"content\"], vector_type=\"k\")\n",
    "i_vec_i = i2v.infer_item_vector(test_items, key=lambda x: x[\"content\"], vector_type=\"i\")\n",
    "\n",
    "print(t_vec.shape) # == torch.Size([2, 23, 128])\n",
    "print(i_vec_k.shape) # == torch.Size([2, 128])\n",
    "print(i_vec_i.shape) # == torch.Size([2, 128])\n",
    "\n",
    "t_vec = i2v.infer_token_vector(test_items[0], key=lambda x: x[\"content\"])\n",
    "i_vec_k = i2v.infer_item_vector(test_items[0], key=lambda x: x[\"content\"], vector_type=\"k\")\n",
    "i_vec_i = i2v.infer_item_vector(test_items, key=lambda x: x[\"content\"], vector_type=\"i\")\n",
    "\n",
    "print(t_vec.shape) # == torch.Size([1, 11, 128])\n",
    "print(i_vec_k.shape) # == torch.Size([1, 128])\n",
    "print(i_vec_i.shape) # == torch.Size([2, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 使用DisenQTokenizer先分词，再用T2V向量化\n",
    "#### 使用DisenQTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save words(trim_min_count=1): 27/27 = 1.0000 with frequency 31/31=1.0000\n",
      "{'content_idx': tensor([[20, 10, 14,  4, 28, 11,  3]]), 'content_len': tensor([7])}\n",
      "\n",
      "{'content_idx': tensor([[20, 10, 14,  4, 28, 11,  3,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "          2,  2,  2,  2,  2,  2],\n",
      "        [13, 12,  5,  7, 29, 21,  6, 22, 23, 24, 25, 26, 12, 18, 17, 16, 22, 15,\n",
      "         22, 27, 22, 19,  9,  8]]), 'content_len': tensor([ 7, 24])}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 在Bert-base-chinese的基础上初始化tokenizer\n",
    "tokenizer = DisenQTokenizer(max_length=250, tokenize_method=\"space\")\n",
    "\n",
    "# 对题目文本进行令牌化\n",
    "items = [\n",
    "    \"有 公 式 $\\\\FormFigureID{wrong1?}$ ，如 图 $\\\\FigureID{088f15ea-xxx}$\",\n",
    "    \"已知 圆 $x^{2}+y^{2}-6 x=0$ ，过 点 (1,2) 的 直 线 被 该 圆 所 截 得 的 弦 的 长度 的 最小 值 为\"\n",
    "]\n",
    "tokenizer.set_vocab(items, silent=False)\n",
    "\n",
    "# 可以对单个题目进行令牌化\n",
    "print(tokenizer(items[0]))\n",
    "print()\n",
    "\n",
    "# 也可以对题目列表进行令牌化\n",
    "token_items = tokenizer(items)\n",
    "print(token_items)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['有', '公', '式', '$\\\\FormFigureID{wrong1?}$', '，如', '图', '$\\\\FigureID{088f15ea-xxx}$']\n",
      "[['有', '公', '式', '$\\\\FormFigureID{wrong1?}$', '，如', '图', '$\\\\FigureID{088f15ea-xxx}$'], ['已知', '圆', '$x^{2}+y^{2}-6', 'x=0$', '，过', '点', '(1,2)', '的', '直', '线', '被', '该', '圆', '所', '截', '得', '的', '弦', '的', '长度', '的', '最小', '值', '为']]\n"
     ]
    }
   ],
   "source": [
    "# 可以使用tokenize方法查看令牌化后的文本\n",
    "print(tokenizer.tokenize(items[0]))\n",
    "print(tokenizer.tokenize(items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用T2V加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128]) torch.Size([2, 128])\n",
      "torch.Size([2, 24, 128])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pretrained_dir = f\"{BASE_DIR}/examples/test_model/data/disenq\"\n",
    "t2v = DisenQModel(pretrained_dir)\n",
    "\n",
    "token_items = tokenizer(items)\n",
    "\n",
    "# 获得句表征和词表征\n",
    "t_vec, i_vec_k, i_vec_i = t2v(token_items)\n",
    "print(i_vec_k.shape, i_vec_i.shape)\n",
    "print(t_vec.shape)\n",
    "print()\n",
    "\n",
    "# 获得指定表征\n",
    "i_vec_k, i_vec_i = t2v.infer_vector(token_items, vector_type=\"k\")\n",
    "t_vec = t2v.infer_tokens(token_items)\n",
    "\n",
    "i_vec_k = t2v.infer_vector(token_items, vector_type=\"k\")\n",
    "i_vec_i = t2v.infer_vector(token_items, vector_type=\"i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 使用EduNLP中公开的预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EduNLP, INFO model_dir: ..\\..\\examples\\test_model\\data\\disenq_pub\\disenq_pub_128\n",
      "EduNLP, INFO Use pretrained t2v model disenq_pub_128\n",
      "downloader, INFO http://base.ustc.edu.cn/data/model_zoo/modelhub/disenq_public/1/disenq_pub_128.zip is saved as ..\\..\\examples\\test_model\\data\\disenq_pub\\disenq_pub_128.zip\n",
      "downloader, INFO file existed, skipped\n"
     ]
    }
   ],
   "source": [
    "# 获取公开的预训练模型\n",
    "pretrained_dir = f\"{BASE_DIR}/examples/test_model/data/disenq_pub\"\n",
    "i2v = get_pretrained_i2v(\"disenq_pub_128\", model_dir=pretrained_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128]) torch.Size([2, 128])\n",
      "torch.Size([2, 24, 128])\n",
      "\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 24, 128])\n",
      "\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 24, 128])\n"
     ]
    }
   ],
   "source": [
    "test_items = [\n",
    "    \"有 公 式 $\\\\FormFigureID{wrong1?}$ ，如 图 $\\\\FigureID{088f15ea-xxx}$\",\n",
    "    \"已知 圆 $x^{2}+y^{2}-6 x=0$ ，过 点 (1,2) 的 直 线 被 该 圆 所 截 得 的 弦 的 长度 的 最小 值 为\"\n",
    "]\n",
    "\n",
    "# 获得句表征和词表征\n",
    "i_vec, t_vec = i2v(test_items)\n",
    "print(i_vec[0].shape, i_vec[1].shape)\n",
    "print(t_vec.shape)\n",
    "print()\n",
    "\n",
    "i_vec_k, t_vec = i2v(test_items, vector_type=\"k\")\n",
    "print(i_vec_k.shape)\n",
    "print(t_vec.shape)\n",
    "print()\n",
    "\n",
    "# 获得指定表征\n",
    "i_vec_k = i2v.infer_item_vector(test_items, vector_type=\"k\")\n",
    "i_vec_i = i2v.infer_item_vector(test_items, vector_type=\"i\")\n",
    "t_vec = i2v.infer_token_vector(test_items)\n",
    "\n",
    "print(i_vec_k.shape)\n",
    "print(i_vec_i.shape)\n",
    "print(t_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text tokenization method of pretrained i2v:  space\n",
      "Reset the text tokenization method of pretrained i2v:  pure_text\n",
      "torch.Size([2, 128]) torch.Size([2, 128])\n",
      "torch.Size([2, 25, 128])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_items2 = [\n",
    "    \"有公式$\\\\FormFigureID{wrong1?}$，如图$\\\\FigureID{088f15ea-xxx}$,\\\n",
    "    若$x,y$满足约束条件公式$\\\\FormFigureBase64{wrong2?}$,$\\\\SIFSep$，则$z=x+7 y$的最大值为$\\\\SIFBlank$\",\n",
    "    \"已知圆$x^{2}+y^{2}-6 x=0$，过点(1,2)的直线被该圆所截得的弦的长度的最小值为\"\n",
    "]\n",
    "\n",
    "print(\"The text tokenization method of pretrained i2v: \",i2v.tokenizer.tokenize_method)\n",
    "\n",
    "# if the test data is note the same formation as train data, you can change tokenzer_method! But it's not recommended.\n",
    "i2v.tokenizer.set_text_tokenizer(\"pure_text\")\n",
    "print(\"Reset the text tokenization method of pretrained i2v: \",i2v.tokenizer.tokenize_method)\n",
    "\n",
    "i_vec, t_vec = i2v(test_items2)\n",
    "print(i_vec[0].shape, i_vec[1].shape)\n",
    "print(t_vec.shape)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "776957673adb719a00031a24ed5efd2fa5ce8a13405e5193f8d278edd3805d55"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
